How to reindex Atlas Hive Metadata into Solr collections?
===========================================

Description: While uploading data into Atlas metadata using the import-hive.sh from Hive DBs and tables, it seems that data cannot be indexed into Atlas Solr collection Index. The WUI from Atlas search (based search) is unable to retrieve the database-specific information to build tags. However, while executing the REST call, the metadata information is returned.

What causes this issue? The REST call gets the data directly from the HBase Atlas titan table, while the Atlas WUI uses the Atlas Solrs collection Index.

Currently, in HDP 2.6.4, a tool (attached) is being built for the reindexing. The tool is still on very early stages of development and needs to be tested before executing in Productions environments.

Solution: There are two ways to solve the issue.

Proposal A:
    Shut down Atlas service.
    Wipe the HBase titan table regarding Atlas.
    Wipe the collections Index which is generated by Atlas.
    Restart Atlas, the titan table, and collections should be regenerated.
    Re-execute the import-hive .sh, to download the metadata from Hive into HBase and Solr cluster.

Proposal B:
    Setup titan-0.5.4 in the host where Atlas is installed.
    WARNING: Please execute this in a testing environment first. Any issue or question please update the case.

    ref: https://github.com/apache/atlas/tree/branch-0.8/tools/atlas-index-repair-kit
    ref: https://community.hortonworks.com/articles/61274/import-hive-metadata-into-atlas.html

    1. Download titan-0.5.4 Hadoop-2 distribution from http://s3.thinkaurelius.com/downloads/titan/titan-0.5.4-hadoop2.zip
        wget http://s3.thinkaurelius.com/downloads/titan/titan-0.5.4-hadoop2.zip
        unzip titan-0.5.4-hadoop2.zip
        cd titan-0.5.4-hadoop2
        # Current directory: ~/titan-0.5.4-hadoop2/

    2. Add Atlas index repair kit to titan-0.5.4 installation, by running the following commands:
        wget https://github.com/apache/atlas/raw/branch-0.8/tools/atlas-index-repair-kit/atlas-index-repair-kit.tar.gz
        tar xvzf atlas-index-repair-kit.tar.gz

    3. Update atlas-conf/atlas-titan.properties with details to connect to data-store (like HBase).
        For example:
        vim atlas-conf/atlas-titan.properties
            storage.backend=hbase
            # storage.hostname = HBase server host FQDN
            storage.hostname=c239-node4.squadron-labs.com
            # storage.hbase.table = Ambari UI > Atlas > Configs > Advanced application-properties > atlas.graph.storage.hbase.table
            storage.hbase.table=atlas_titan
            # index.search.backend = Ambari UI > Atlas > Configs > Advanced > Advanced application-properties > atlas.graph.index.search.backend
            index.search.backend=solr5
            # mindex.search.solr.mode = Ambari UI > Atlas > Configs > Advanced > Advanced application-properties > atlas.graph.index.search.solr.mode
            index.search.solr.mode=cloud
            # index.search.solr.zookeeper-url = Ambari UI > ZooKeeper > Configs > ZooKeeper Server > ZooKeeper Server hosts. Only one is needed.
            index.search.solr.zookeeper-url=c239-node2.squadron-labs.com:2181/infra-solr

        3.1 Also, if you are running a kerberized cluster, set up the following values as well.
            hbase.security.authentication=kerberos
            hbase.security.authorization=true
            hbase.rpc.engine=org.apache.hadoop.hbase.ipc.SecureRpcEngine

    4. Update bin/atlas-gremlin.sh to set the following variables at the top. For example:
        Note: use the command `find -L /usr/hdp/current/ -name webapp|grep atlas-server` to get the right path to webapp. you should get the value by running the following command:
        ATLAS_WEBAPP_DIR=$(find -L /usr/hdp/current/ -name webapp|grep atlas-server)
        echo $ATLAS_WEBAPP_DIR
            /usr/hdp/current/atlas-server/server/webapp  <<< Use this value.
        vim bin/atlas-gremlin.sh
            ATLAS_WEBAPP_DIR=/usr/hdp/current/atlas-server/server/webapp
            STORE_CONF_DIR=/etc/hbase/conf

    5. Ensure the home directory for 'atlas' user exists in HDFS and this directory is owned by 'atlas' user
        su -l hdfs -c "kinit -kt /etc/security/keytabs/spnego.service.keytab HTTP/xxx.hortonworks.com &&  hdfs groups atlas"
            Expected output: atlas:hadoop

        su -l hdfs -c "kinit -kt /etc/security/keytabs/spnego.service.keytab HTTP/xxx.hortonworks.com &&  hdfs dfs -ls /user"
            Found 2 items
            drwxrwx---   - ambari-qa hdfs          0 2018-06-27 22:29 /user/ambari-qa
            drwxr-xr-x   - hbase     hdfs          0 2018-06-22 21:14 /user/hbase
        Note: I need to create the diretory.

        If not using Kerberos, you can do it like this:
            su - hdfs
            hdfs dfs -mkdir /user/atlas && hdfs dfs -chown -R atlas:hdfs /user/atlas
        Else:
            su -l hdfs -c "kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs-krbcrossrealm && hdfs dfs -mkdir /user/atlas && hdfs dfs -chown -R atlas:hdfs /user/atlas"
            su -l hdfs -c "kinit -kt /etc/security/keytabs/spnego.service.keytab HTTP/xxx.hortonworks.com &&  hdfs dfs -ls /user/"
                Found 4 items
                drwxrwx---   - ambari-qa hdfs          0 2018-06-27 22:29 /user/ambari-qa
                drwxr-xr-x   - atlas     hdfs          0 2018-06-29 18:43 /user/atlas
                drwxr-xr-x   - hbase     hdfs          0 2018-06-22 21:14 /user/hbase
                drwx------   - hdfs      hdfs          0 2018-06-29 18:40 /user/hdfs
            Note: the /user/atlas directory is created and given the right ownership.

    6. Copy necessary configuration files from the deployment to atlas-conf folder. For example:
        hadoop-client/conf/core-site.xml, hdfs-site.xml, yarn-site.xml
        ambari-infra-solr/conf/infra_solr_jaas.conf, security.json << If using kerberos

        Note: Use the ln commands below to create all needed symlink for this step.
            cd atlas-conf
            # Current directory: ~/titan-0.5.4-hadoop2/atlas-conf/
            ln -s /etc/hadoop/conf/hdfs-site.xml hdfs-site.xml
            ln -s /etc/hadoop/conf/core-site.xml core-site.xml
            ln -s /etc/hadoop/conf/yarn-site.xml yarn-site.xml
            ln -s /etc/ambari-infra-solr/conf/infra_solr_jaas.conf infra_solr_jaas.conf # <<<  When running a secure cluster.
            ln -s /etc/ambari-infra-solr/conf/security.json security.json # <<<  When running a secure cluster.

    7. Update bin/atlas-gremlin.sh to set the following variable at the top if you are running a kerberized cluster. For example:
        JAAS_CONF_FILE=infra_solr_jaas.conf

    8. WARNING: Before running the tool, STOP Atlas and let's check the Solr env, then follow the 8a steps if your cluster is kerberized or 8b otherwise:
        8a.1. Kinit as atlas user with the command line,  before running this utility:
            kinit -kt /etc/security/keytabs/atlas.service.keytab atlas/xxx.hortonworks.com@HORTONWORKS.COM

        8a.2. Check for the available collections in the Solr environment:
            curl -k --negotiate -u : "http://xxx.hortonworks.com:8886/solr/admin/collections?action=LIST&wt=json"
                {"responseHeader":{"status":0,"QTime":0},"collections":["fulltext_index","vertex_index","edge_index"]}

        8a.3. Remove the indexed data from the collections:
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/fulltext_index/update?stream.body=<delete><query>*</query></delete>&commit=true&async=1101" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/vertex_index/update?stream.body=<delete><query>*</query></delete>&commit=true&async=1102" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/edge_index/update?stream.body=<delete><query>*</query></delete>&commit=true&&async=1103"

        8a.4. Get status on the executing jobs:
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/admin/collections?action=REQUESTSTATUS&requestid=1101&wt=json" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/admin/collections?action=REQUESTSTATUS&requestid=1102&wt=json" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/admin/collections?action=REQUESTSTATUS&requestid=1103&wt=json"

        8a.5. Check if all collections are empty:
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/fulltext_index/select?q=*:*&distrib=false&wt=json" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/vertex_index/select?q=*:*&distrib=false&wt=json" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/edge_index/select?q=*:*&distrib=false&wt=json"

        [End of 8a.]
        --
        8b.1. Check the available collections in the Solr environment:
            export INFRASOLRHOST="c239-node3.squadron-labs.com"
            export INFRASOLRPORT=8886
            export USERNAME=admin
            export PASS=admin
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=LIST&wt=json"
                {"responseHeader":{"status":0,"QTime":0},"collections":["fulltext_index","edge_index","vertex_index"]}

        8a.2. Remove the indexed data from the collections:
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/fulltext_index/update?stream.body=<delete><query>*</query></delete>&commit=true&async=1101" && \
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/vertex_index/update?stream.body=<delete><query>*</query></delete>&commit=true&async=1102" && \
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/edge_index/update?stream.body=<delete><query>*</query></delete>&commit=true&&async=1103"
                <?xml version="1.0" encoding="UTF-8"?>
                <response>
                <lst name="responseHeader"><int name="status">0</int><int name="QTime">140</int></lst>
                </response>
                <?xml version="1.0" encoding="UTF-8"?>
                <response>
                <lst name="responseHeader"><int name="status">0</int><int name="QTime">99</int></lst>
                </response>
                <?xml version="1.0" encoding="UTF-8"?>
                <response>
                <lst name="responseHeader"><int name="status">0</int><int name="QTime">5</int></lst>
                </response>

        8a.3. Get status on the executing jobs:
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=1101&wt=json" && \
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=1102&wt=json" && \
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=1103&wt=json"
                {"responseHeader":{"status":0,"QTime":2},"status":{"state":"notfound","msg":"Did not find [1101] in any tasks queue"}}
                {"responseHeader":{"status":0,"QTime":1},"status":{"state":"notfound","msg":"Did not find [1102] in any tasks queue"}}
                {"responseHeader":{"status":0,"QTime":1},"status":{"state":"notfound","msg":"Did not find [1103] in any tasks queue"}}

        8a.4. Check if all collections are empty:
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/fulltext_index/select?q=*:*&distrib=false&wt=json" && \
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/vertex_index/select?q=*:*&distrib=false&wt=json" && \
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/edge_index/select?q=*:*&distrib=false&wt=json"
                {"responseHeader":{"status":0,"QTime":7,"params":{"q":"*:*","distrib":"false","wt":"json"}},"response":{"numFound":0,"start":0,"docs":[]}}
                {"responseHeader":{"status":0,"QTime":0,"params":{"q":"*:*","distrib":"false","wt":"json"}},"response":{"numFound":0,"start":0,"docs":[]}}
                {"responseHeader":{"status":0,"QTime":0,"params":{"q":"*:*","distrib":"false","wt":"json"}},"response":{"numFound":0,"start":0,"docs":[]}}

    9. Start Gremlin shell by executing the following command:
        cd .. # You should have the bin folder on this same level.
        # Current directory: ~/titan-0.5.4-hadoop2/
        export PATH=$PATH:$(dirname $(ps -ef | grep hive | head -1 | awk '{print $8}')) # Get the java path for the cluster services.
        bin/atlas-gremlin.sh bin/atlas-index-repair.groovy
                    \,,,/
                    (o o)
            -----oOOo-(_)-oOOo-----
            java.util.prefs.FileSystemPreferences$1 run
            INFO: Created user preferences directory.
            ==>true
            ==>true
            ==>true
            gremlin>

    10. Start index repair by entering the following in the Gremlin shell:
        repairAtlasIndex("atlas-conf/atlas-titan.properties")
            2019-01-14 21:07:21.147 Initializing graph..
            2019-01-14 21:07:52.451 Graph initialized!
            2019-01-14 21:07:52.458 Processing index: vertex_index
            2019-01-14 21:07:52.458     Start Index : 0
            2019-01-14 21:07:52.458     End Index   : 38
            2019-01-14 21:07:52.459     Chunk Count : 32
            2019-01-14 21:07:52.459     Batch Size  : 2
            2019-01-14 21:07:52.469 Batch Id: [30-32] thread start
            [...]
            2019-01-14 21:07:54.394 Batch Id: [28-30]: {28-30} commit
            2019-01-14 21:07:54.394 Batch Id: [28-30] thread end. Time taken: 31ms
            2019-01-14 21:07:54.395 Restore complete: edge_index. Time taken: 36ms
            2019-01-14 21:07:54.751 Graph shutdown!
            ==>null
            gremlin> quit
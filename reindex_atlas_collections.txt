How to reindex Atlas Hive Metadata into Solr collections?
===========================================

Description: While uploading data into Atlas metadata using the import-hive.sh from Hive DBs and tables, it seems that data cannot be indexed into Atlas Solr collection Index. The WUI from Atlas search (based search) is unable to retrieve the database-specific information to build tags. However, while executing the REST call, the metadata information is returned.

What causes this issue? The REST call gets the data directly from the HBase Atlas titan table, while the Atlas WUI uses the Atlas Solrs collection Index.

Currently, in HDP 2.6.4, a tool (attached) is being built for the reindexing. The tool is still on very early stages of development and needs to be tested before executing in Productions environments.

Solution: There are two ways to solve the issue.

Proposal A:
    Shut down Atlas service.
    Wipe the HBase titan table regarding Atlas.
    Wipe the collections Index which is generated by Atlas.
    Restart Atlas, the titan table, and collections should be regenerated.
    Re-execute the import-hive .sh, to download the metadata from Hive into HBase and Solr cluster.

Proposal B:
    Setup titan-0.5.4 in the host where Atlas is installed.
    WARNING: Please execute this in a testing environment first. Any issue or question please update the case.

    ref: https://github.com/apache/atlas/tree/branch-0.8/tools/atlas-index-repair-kit
    ref: https://community.hortonworks.com/articles/61274/import-hive-metadata-into-atlas.html

    1. Download titan-0.5.4 Hadoop-2 distribution from http://s3.thinkaurelius.com/downloads/titan/titan-0.5.4-hadoop2.zip
        wget http://s3.thinkaurelius.com/downloads/titan/titan-0.5.4-hadoop2.zip
        unzip titan-0.5.4-hadoop2.zip
        cd titan-0.5.4-hadoop2
        # Current directory: ~/titan-0.5.4-hadoop2/

    2. Add Atlas index repair kit to titan-0.5.4 installation, by running the following commands:
        wget https://github.com/apache/atlas/raw/branch-0.8/tools/atlas-index-repair-kit/atlas-index-repair-kit.tar.gz
        tar xvzf atlas-index-repair-kit.tar.gz

    3. Update atlas-conf/atlas-titan.properties with details to connect to data-store (like HBase).
        For example:
        vim atlas-conf/atlas-titan.properties
            storage.backend=hbase
            # HBase server host FQDN. Ambari UI > HBase > Configs > Advanced > HBase Master > HBase Master host
            storage.hostname=example.cloudera.com
            # Ambari UI > Atlas > Configs > Advanced application-properties > atlas.graph.storage.hbase.table
            storage.hbase.table=atlas_titan
            # Ambari UI > Atlas > Configs > Advanced > Advanced application-properties > atlas.graph.index.search.backend
            index.search.backend=solr5
            # Ambari UI > Atlas > Configs > Advanced > Advanced application-properties > atlas.graph.index.search.solr.mode
            index.search.solr.mode=cloud
            # Ambari UI > Atlas > Configs > Advanced > Advanced application-properties > atlas.graph.index.search.solr.zookeeper-url. Only one is
            index.search.solr.zookeeper-url=example.cloudera.com:2181/infra-solr

        3.1 Also, if you are running a kerberized cluster, set up the following values as well.
            hbase.security.authentication=kerberos
            hbase.security.authorization=true
            hbase.rpc.engine=org.apache.hadoop.hbase.ipc.SecureRpcEngine

    4. Update bin/atlas-gremlin.sh and set the following variables: ATLAS_WEBAPP_DIR, STORE_CONF_DIR, JAAS_CONF_FILE. For example:
        vim bin/atlas-gremlin.sh
            ATLAS_WEBAPP_DIR=/usr/hdp/current/atlas-server/server/webapp
            STORE_CONF_DIR=/etc/hbase/conf

        If running a kerberized cluster:
            JAAS_CONF_FILE=infra_solr_jaas.conf

        Note: use the command `find -L /usr/hdp/current/ -name webapp|grep atlas-server` to get the right path to webapp. you should get the value by running the following command:
        ATLAS_WEBAPP_DIR=$(find -L /usr/hdp/current/ -name webapp|grep atlas-server)
        echo $ATLAS_WEBAPP_DIR
            /usr/hdp/current/atlas-server/server/webapp  <<< Use this value.

    5. Ensure the home directory for 'atlas' user exists in HDFS and this directory is owned by 'atlas' user
        su -l hdfs -c "kinit -kt /etc/security/keytabs/spnego.service.keytab HTTP/$(hostname -f) &&  hdfs groups atlas"
            Expected output:
                atlas : hadoop

        su -l hdfs -c "kinit -kt /etc/security/keytabs/spnego.service.keytab HTTP/$(hostname -f) &&  hdfs dfs -ls /user"
            Example output:
                Found 4 items
                drwxrwx---   - ambari-qa hdfs          0 2019-05-24 21:29 /user/ambari-qa
                drwxr-xr-x   - hbase     hdfs          0 2019-05-24 21:14 /user/hbase
                drwxr-xr-x   - hcat      hdfs          0 2019-05-24 21:31 /user/hcat
                drwxr-xr-x   - hive      hdfs          0 2019-05-24 21:32 /user/hive
            Note: the atlas user directory does not exist yet.

        If using Kerberos, you can do it like this:
            su -l hdfs -c "kinit -kt /etc/security/keytabs/hdfs.headless.keytab $(klist -k  /etc/security/keytabs/hdfs.headless.keytab | tail -n 1 | awk '{print $2}') && hdfs dfs -mkdir /user/atlas && hdfs dfs -chown -R atlas:hdfs /user/atlas"
            su -l hdfs -c "kinit -kt /etc/security/keytabs/spnego.service.keytab HTTP/$(hostname -f) &&  hdfs dfs -ls /user/"
            Example output:
                Found 5 items
                drwxrwx---   - ambari-qa hdfs          0 2019-05-24 21:29 /user/ambari-qa
                drwxr-xr-x   - atlas     hdfs          0 2019-05-27 19:18 /user/atlas       <<< New directory
                drwxr-xr-x   - hbase     hdfs          0 2019-05-24 21:14 /user/hbase
                drwxr-xr-x   - hcat      hdfs          0 2019-05-24 21:31 /user/hcat
                drwxr-xr-x   - hive      hdfs          0 2019-05-24 21:32 /user/hive
            Note: the atlas user directory should be in the list now with the atlas:hdfs ownership.
        Else:
            su - hdfs
            su -l hdfs -c "hdfs dfs -mkdir /user/atlas && hdfs dfs -chown -R atlas:hdfs /user/atlas"

    6. Copy necessary configuration files from the deployment to atlas-conf folder: hadoop-client/conf/core-site.xml, hdfs-site.xml, yarn-site.xml; and ambari-infra-solr/conf/infra_solr_jaas.conf, security.json if using kerberos.
        Note: you can use ln commands to create all needed symlinks for this step, for example:
            cd atlas-conf
            # Current directory: ~/titan-0.5.4-hadoop2/atlas-conf/
            ln -s /etc/hadoop/conf/hdfs-site.xml hdfs-site.xml
            ln -s /etc/hadoop/conf/core-site.xml core-site.xml
            ln -s /etc/hadoop/conf/yarn-site.xml yarn-site.xml
            ln -s /etc/ambari-infra-solr/conf/infra_solr_jaas.conf infra_solr_jaas.conf # <<<  When running a secure cluster.
            ln -s /etc/ambari-infra-solr/conf/security.json security.json # <<<  When running a secure cluster.

    8. WARNING: Before running the tool, STOP Atlas and check the Solr env, then follow the 8a steps if your cluster is kerberized or 8b otherwise:
	Set these values:

	INFRASOLRHOST=example.cloudera.com # The FQDN for the Ambari Infra Solr server host. Ambari Infra > Summary > Infra Solr Instance
	INFRASOLRPORT=8886 # The Ambari Infra Solr server port. Ambari Infra > Configs > Advanced > Advanced infra-solr-env > Infra Solr port

	== If you are running Kerberos on your cluster, run the following 8a steps, else, continue with 8b ==
        8a.1. Kinit as atlas user with the command line,  before running this utility:
            kinit -kt /etc/security/keytabs/atlas.service.keytab atlas/example.cloudera.com@CLOUDERA.COM
            kinit -kt /etc/security/keytabs/atlas.service.keytab atlas/$(hostname -f)

        8a.2. Check for the available collections in the Solr environment:
            curl -k --negotiate -u : "http://xxx.hortonworks.com:8886/solr/admin/collections?action=LIST&wt=json"
                {"responseHeader":{"status":0,"QTime":0},"collections":["fulltext_index","vertex_index","edge_index"]}

        8a.3. Remove the indexed data from the collections:
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/fulltext_index/update?stream.body=<delete><query>*</query></delete>&commit=true&async=1101" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/vertex_index/update?stream.body=<delete><query>*</query></delete>&commit=true&async=1102" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/edge_index/update?stream.body=<delete><query>*</query></delete>&commit=true&&async=1103"

        8a.4. Get status on the executing jobs:
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/admin/collections?action=REQUESTSTATUS&requestid=1101&wt=json" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/admin/collections?action=REQUESTSTATUS&requestid=1102&wt=json" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/admin/collections?action=REQUESTSTATUS&requestid=1103&wt=json"

        8a.5. Check if all collections are empty:
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/fulltext_index/select?q=*:*&distrib=false&wt=json" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/vertex_index/select?q=*:*&distrib=false&wt=json" && \
            curl -k --negotiate -u : "http://$(hostname -f):8886/solr/edge_index/select?q=*:*&distrib=false&wt=json"

        [End of 8a.]
        --
        8b.1. Check the available collections in the Solr environment:
            export INFRASOLRHOST="example.cloudera.com"
            export INFRASOLRPORT=8886
            export USERNAME=admin
            export PASS=admin
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=LIST&wt=json"
                # Example response. Please notice the "fulltext_index","vertex_index","edge_index" entries under "collections".
                {"responseHeader":{"status":0,"QTime":0},"collections":["fulltext_index","edge_index","vertex_index"]}

        8a.2. Remove the indexed data from the collections:
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/fulltext_index/update?stream.body=<delete><query>*</query></delete>&commit=true&async=1101" && \
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/vertex_index/update?stream.body=<delete><query>*</query></delete>&commit=true&async=1102" && \
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/edge_index/update?stream.body=<delete><query>*</query></delete>&commit=true&&async=1103"
                <?xml version="1.0" encoding="UTF-8"?>
                <response>
                <lst name="responseHeader"><int name="status">0</int><int name="QTime">140</int></lst>
                </response>
                <?xml version="1.0" encoding="UTF-8"?>
                <response>
                <lst name="responseHeader"><int name="status">0</int><int name="QTime">99</int></lst>
                </response>
                <?xml version="1.0" encoding="UTF-8"?>
                <response>
                <lst name="responseHeader"><int name="status">0</int><int name="QTime">5</int></lst>
                </response>

        8a.3. Get status on the executing jobs:
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=1101&wt=json" && \
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=1102&wt=json" && \
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=1103&wt=json"
                {"responseHeader":{"status":0,"QTime":2},"status":{"state":"notfound","msg":"Did not find [1101] in any tasks queue"}}
                {"responseHeader":{"status":0,"QTime":1},"status":{"state":"notfound","msg":"Did not find [1102] in any tasks queue"}}
                {"responseHeader":{"status":0,"QTime":1},"status":{"state":"notfound","msg":"Did not find [1103] in any tasks queue"}}

        8a.4. Check if all collections are empty:
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/fulltext_index/select?q=*:*&distrib=false&wt=json"
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/vertex_index/select?q=*:*&distrib=false&wt=json"
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "http://$INFRASOLRHOST:$INFRASOLRPORT/solr/edge_index/select?q=*:*&distrib=false&wt=json"
                {"responseHeader":{"status":0,"QTime":7,"params":{"q":"*:*","distrib":"false","wt":"json"}},"response":{"numFound":0,"start":0,"docs":[]}}
                {"responseHeader":{"status":0,"QTime":0,"params":{"q":"*:*","distrib":"false","wt":"json"}},"response":{"numFound":0,"start":0,"docs":[]}}
                {"responseHeader":{"status":0,"QTime":0,"params":{"q":"*:*","distrib":"false","wt":"json"}},"response":{"numFound":0,"start":0,"docs":[]}}

    9. Start Gremlin shell by executing the following command:
        cd .. # You should have the bin folder on this same level.
        # Current directory: ~/titan-0.5.4-hadoop2/
        export PATH=$PATH:$(dirname $(ps -ef | grep hive | head -1 | awk '{print $8}')) # Get the java path for the cluster services.
        bin/atlas-gremlin.sh bin/atlas-index-repair.groovy
                    \,,,/
                    (o o)
            -----oOOo-(_)-oOOo-----
            java.util.prefs.FileSystemPreferences$1 run
            INFO: Created user preferences directory.
            ==>true
            ==>true
            ==>true
            gremlin>

    10. Start index repair by entering the following in the Gremlin shell:
        repairAtlasIndex("atlas-conf/atlas-titan.properties")
            2019-01-14 21:07:21.147 Initializing graph..
            2019-01-14 21:07:52.451 Graph initialized!
            2019-01-14 21:07:52.458 Processing index: vertex_index
            2019-01-14 21:07:52.458     Start Index : 0
            2019-01-14 21:07:52.458     End Index   : 38
            2019-01-14 21:07:52.459     Chunk Count : 32
            2019-01-14 21:07:52.459     Batch Size  : 2
            2019-01-14 21:07:52.469 Batch Id: [30-32] thread start
            [...]
            2019-01-14 21:07:54.394 Batch Id: [28-30]: {28-30} commit
            2019-01-14 21:07:54.394 Batch Id: [28-30] thread end. Time taken: 31ms
            2019-01-14 21:07:54.395 Restore complete: edge_index. Time taken: 36ms
            2019-01-14 21:07:54.751 Graph shutdown!
            ==>null
            gremlin> quit

    11. Start Atlas service from Ambari UI.
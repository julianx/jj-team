How to reindex Atlas Hive Metadata into Solr collections?
===========================================

Description: While uploading data into Atlas metadata using the import-hive.sh from Hive DBs and tables, it seems that data cannot be indexed into Atlas Solr collection Index. The WUI from Atlas search (based search) is unable to retrieve the database-specific information to build tags. However, while executing the REST call, the metadata information is returned.

What causes this issue? The REST call gets the data directly from the HBase Atlas titan table, while the Atlas WUI uses the Atlas Solrs collection Index.

Currently, in HDP 2.6.4, a tool (attached) is being built for the reindexing. The tool is still on very early stages of development and needs to be tested before executing in Productions environments.

Solution: There are two ways to solve the issue.

Proposal A:
    Shut down Atlas service.
    Wipe the HBase titan table regarding Atlas.
    Wipe the collections Index which is generated by Atlas.
    Restart Atlas, the titan table, and collections should be regenerated.
    Re-execute the import-hive .sh, to download the metadata from Hive into HBase and Solr cluster.

Proposal B:
    Setup titan-0.5.4 in the host where Atlas is installed.
    WARNING: Please execute this in a testing environment first. Any issue or question please update the case.

    ref: https://github.com/apache/atlas/tree/branch-0.8/tools/atlas-index-repair-kit
    ref: https://community.hortonworks.com/articles/61274/import-hive-metadata-into-atlas.html

    1. Download titan-0.5.4 Hadoop-2 distribution from http://s3.thinkaurelius.com/downloads/titan/titan-0.5.4-hadoop2.zip
        wget http://s3.thinkaurelius.com/downloads/titan/titan-0.5.4-hadoop2.zip
        unzip titan-0.5.4-hadoop2.zip
        cd titan-0.5.4-hadoop2
        # Current directory: ~/titan-0.5.4-hadoop2/

    2. Add Atlas index repair kit to titan-0.5.4 installation, by running the following commands:
        wget https://github.com/apache/atlas/raw/branch-0.8/tools/atlas-index-repair-kit/atlas-index-repair-kit.tar.gz
        tar xvzf atlas-index-repair-kit.tar.gz

    3. Update atlas-conf/atlas-titan.properties with details to connect to data-store (like HBase).
        For example:
        vim atlas-conf/atlas-titan.properties
            storage.backend=hbase
            # HBase server host FQDN. Ambari UI > HBase > Configs > Advanced > HBase Master > HBase Master host
            storage.hostname=example.cloudera.com
            # Ambari UI > Atlas > Configs > Advanced application-properties > atlas.graph.storage.hbase.table
            storage.hbase.table=atlas_titan
            # Ambari UI > Atlas > Configs > Advanced > Advanced application-properties > atlas.graph.index.search.backend
            index.search.backend=solr5
            # Ambari UI > Atlas > Configs > Advanced > Advanced application-properties > atlas.graph.index.search.solr.mode
            index.search.solr.mode=cloud
            # Ambari UI > Atlas > Configs > Advanced > Advanced application-properties > atlas.graph.index.search.solr.zookeeper-url. Only one is
            index.search.solr.zookeeper-url=example.cloudera.com:2181/infra-solr

        3.1 Also, if you are running a kerberized cluster, set up the following values as well.
            hbase.security.authentication=kerberos
            hbase.security.authorization=true
            hbase.rpc.engine=org.apache.hadoop.hbase.ipc.SecureRpcEngine

    4. Update bin/atlas-gremlin.sh and set the following variables: ATLAS_WEBAPP_DIR, STORE_CONF_DIR, JAAS_CONF_FILE. For example:
        vim bin/atlas-gremlin.sh
            ATLAS_WEBAPP_DIR=/usr/hdp/current/atlas-server/server/webapp
            STORE_CONF_DIR=/etc/hbase/conf

        If running a kerberized cluster:
            JAAS_CONF_FILE=infra_solr_jaas.conf

        Note: use the command `find -L /usr/hdp/current/ -name webapp|grep atlas-server` to get the right path to webapp. you should get the value by running the following command:
        ATLAS_WEBAPP_DIR=$(find -L /usr/hdp/current/ -name webapp|grep atlas-server)
        echo $ATLAS_WEBAPP_DIR
            /usr/hdp/current/atlas-server/server/webapp  <<< Use this value.

    5. Ensure the home directory for 'atlas' user exists in HDFS and this directory is owned by 'atlas' user
        su -l hdfs -c "kinit -kt /etc/security/keytabs/spnego.service.keytab HTTP/$(hostname -f) &&  hdfs groups atlas"
            Expected output:
                atlas : hadoop

        su -l hdfs -c "kinit -kt /etc/security/keytabs/spnego.service.keytab HTTP/$(hostname -f) &&  hdfs dfs -ls /user"
            Example output:
                Found 4 items
                drwxrwx---   - ambari-qa hdfs          0 2019-05-24 21:29 /user/ambari-qa
                drwxr-xr-x   - hbase     hdfs          0 2019-05-24 21:14 /user/hbase
                drwxr-xr-x   - hcat      hdfs          0 2019-05-24 21:31 /user/hcat
                drwxr-xr-x   - hive      hdfs          0 2019-05-24 21:32 /user/hive
            Note: the atlas user directory does not exist yet.

        If using Kerberos, you can do it like this:
            su -l hdfs -c "kinit -kt /etc/security/keytabs/hdfs.headless.keytab $(klist -k  /etc/security/keytabs/hdfs.headless.keytab | tail -n 1 | awk '{print $2}') && hdfs dfs -mkdir /user/atlas && hdfs dfs -chown -R atlas:hdfs /user/atlas"
            su -l hdfs -c "kinit -kt /etc/security/keytabs/spnego.service.keytab HTTP/$(hostname -f) &&  hdfs dfs -ls /user/"
            Example output:
                Found 5 items
                drwxrwx---   - ambari-qa hdfs          0 2019-05-24 21:29 /user/ambari-qa
                drwxr-xr-x   - atlas     hdfs          0 2019-05-27 19:18 /user/atlas       <<< New directory
                drwxr-xr-x   - hbase     hdfs          0 2019-05-24 21:14 /user/hbase
                drwxr-xr-x   - hcat      hdfs          0 2019-05-24 21:31 /user/hcat
                drwxr-xr-x   - hive      hdfs          0 2019-05-24 21:32 /user/hive
            Note: the atlas user directory should be in the list now with the atlas:hdfs ownership.
        Else:
            su - hdfs
            su -l hdfs -c "hdfs dfs -mkdir /user/atlas && hdfs dfs -chown -R atlas:hdfs /user/atlas"

    6. Copy necessary configuration files from the deployment to atlas-conf folder: hadoop-client/conf/core-site.xml, hdfs-site.xml, yarn-site.xml; and ambari-infra-solr/conf/infra_solr_jaas.conf, security.json if using kerberos.
        Note: you can use ln commands to create all needed symlinks for this step, for example:
            cd atlas-conf
            # Current directory: ~/titan-0.5.4-hadoop2/atlas-conf/
            ln -s /etc/hadoop/conf/hdfs-site.xml hdfs-site.xml
            ln -s /etc/hadoop/conf/core-site.xml core-site.xml
            ln -s /etc/hadoop/conf/yarn-site.xml yarn-site.xml
            ln -s /etc/ambari-infra-solr/conf/infra_solr_jaas.conf infra_solr_jaas.conf # <<<  When running a secure cluster.
            ln -s /etc/ambari-infra-solr/conf/security.json security.json # <<<  When running a secure cluster.

    8. WARNING: Before running the tool, STOP Atlas and check the Solr env, then follow the 8a steps if your cluster is kerberized or 8b otherwise:
	Set these values:

    INFRASOLRHTTP=http # https
	INFRASOLRHOST=example.cloudera.com # The FQDN for the Ambari Infra Solr server host. Ambari Infra > Summary > Infra Solr Instance
	INFRASOLRPORT=8886 # The Ambari Infra Solr server port. Ambari Infra > Configs > Advanced > Advanced infra-solr-env > Infra Solr port

	== If you are running Kerberos on your cluster, run the following 8a steps, else, continue with 8b ==
        8a.1. Kinit as atlas user with the command line,  before running this utility:
            # For example: kinit -kt /etc/security/keytabs/atlas.service.keytab atlas/example.cloudera.com@CLOUDERA.COM
            kinit -kt /etc/security/keytabs/atlas.service.keytab atlas/$(hostname -f)

        8a.2. Check for the available collections in the Solr environment. They should be available for the next step, deleting their contents.
            curl -k --negotiate -u : "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=LIST&wt=json"
                # Example response. Please notice the "fulltext_index","vertex_index","edge_index" entries under "collections".
                #   {"responseHeader":{
                #       "status":0,
                #       "QTime":0},
                #   "collections":["edge_index",    <<< Edge index
                #       "vertex_index",             <<< Vertex index
                #       "hadoop_logs",
                #       "history",
                #       "fulltext_index",           <<< Full text
                #       "audit_logs"]}

        8a.3. Remove the indexed data from the Atlas collections:
            curl -k --negotiate -u : "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/fulltext_index/update?commit=true&async=delete_fulltext_index" -H "Content-Type: text/xml" --data-binary "<delete><query>*</query></delete>"
            curl -k --negotiate -u : "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/vertex_index/update?commit=true&async=delete_vertex_index" -H "Content-Type: text/xml" --data-binary "<delete><query>*</query></delete>"
            curl -k --negotiate -u : "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/edge_index/update?commit=true&async=delete_edge_index" -H "Content-Type: text/xml" --data-binary "<delete><query>*</query></delete>"
                # Example response:
                #   {"responseHeader":{
                #       "status":0,
                #       "QTime":6},
                #   "requestid":"delete_vertex_index"}

        8a.4. Get status on the executing jobs:
            curl -k --negotiate -u : "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=delete_fulltext_index&wt=json"
            curl -k --negotiate -u : "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=delete_vertex_index&wt=json"
            curl -k --negotiate -u : "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=delete_edge_index&wt=json"
                # Example response. If the queries already finished, they should show as completed:
                #   "delete_edge_index9227581747905217":{
                #       "responseHeader":{
                #       "status":0,
                #       "QTime":0},
                #   "STATUS":"completed",            <<< Status: completed.

        8a.5. Check if all collections are empty:
            curl -k --negotiate -u : "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/fulltext_index/select?q=*:*&distrib=false&wt=json"
            curl -k --negotiate -u : "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/vertex_index/select?q=*:*&distrib=false&wt=json"
            curl -k --negotiate -u : "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/edge_index/select?q=*:*&distrib=false&wt=json"
                # Example response. The number of records found for those collections is now zero, as expected.
                #   {"responseHeader":{"status":0,"QTime":7,"params":{"q":"*:*","distrib":"false","wt":"json"}},"response":{"numFound":0,"start":0,"docs":[]}}

        [End of 8a.]
        --
        8b.1. Check the available collections in the Solr environment:
            export INFRASOLRHOST="example.cloudera.com"
            export INFRASOLRPORT=8886
            export USERNAME=admin
            export PASS=admin
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=LIST&wt=json"
                # Example response. Please notice the "fulltext_index","vertex_index","edge_index" entries under "collections".
                #   {"responseHeader":{
                #       "status":0,
                #       "QTime":0},
                #   "collections":["edge_index",    <<< Edge index
                #       "vertex_index",             <<< Vertex index
                #       "hadoop_logs",
                #       "history",
                #       "fulltext_index",           <<< Full text
                #       "audit_logs"]}

        8a.2. Remove the indexed data from the collections:
            curl -u $USERNAME -p "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/fulltext_index/update?commit=true&async=delete_fulltext_index" -H "Content-Type: text/xml" --data-binary "<delete><query>*</query></delete>"
            curl -u $USERNAME -p "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/vertex_index/update?commit=true&async=delete_vertex_index" -H "Content-Type: text/xml" --data-binary "<delete><query>*</query></delete>"
            curl -u $USERNAME -p "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/edge_index/update?commit=true&async=delete_edge_index" -H "Content-Type: text/xml" --data-binary "<delete><query>*</query></delete>"
                # Example response:
                #   {"responseHeader":{
                #       "status":0,
                #       "QTime":6},
                #   "requestid":"delete_vertex_index"}

        8a.3. Get status on the executing jobs:
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=delete_fulltext_index&wt=json"
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=delete_vertex_index&wt=json"
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/admin/collections?action=REQUESTSTATUS&requestid=delete_edge_index&wt=json"
                # Example response. If the queries already finished, they should show as completed for the field "STATUS":
                #   "delete_edge_index9227581747905217":{
                #       "responseHeader":{
                #       "status":0,
                #       "QTime":0},
                #   "STATUS":"completed", <<< Status: completed.

        8a.4. Check if all collections are empty:
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/fulltext_index/select?q=*:*&distrib=false&wt=json"
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/vertex_index/select?q=*:*&distrib=false&wt=json"
            curl -u $USERNAME:$PASS -H 'X-Requested-By: ambari' "$INFRASOLRHTTP://$INFRASOLRHOST:$INFRASOLRPORT/solr/edge_index/select?q=*:*&distrib=false&wt=json"
                # Example response. The number of records found for those collections is now zero, as expected.
                #   {"responseHeader":{"status":0,"QTime":7,"params":{"q":"*:*","distrib":"false","wt":"json"}},"response":{"numFound":0,"start":0,"docs":[]}}

    9. Start Gremlin shell by executing the following command:
        cd .. # You should have the bin folder on this same level.
        # Current directory: ~/titan-0.5.4-hadoop2/
        export PATH=$PATH:$(dirname $(ps -ef | grep hive | head -1 | awk '{print $8}')) # Get the java path for the cluster services.
        bin/atlas-gremlin.sh bin/atlas-index-repair.groovy
            # Example output:
            #             \,,,/
            #             (o o)
            #    -----oOOo-(_)-oOOo-----
            #    java.util.prefs.FileSystemPreferences$1 run
            #    INFO: Created user preferences directory.
            #    ==>true
            #    ==>true
            #    ==>true
            #    gremlin>

    10. Start index repair by entering the following in the Gremlin shell:
        repairAtlasIndex("atlas-conf/atlas-titan.properties")
            # Example output:
            #    Initializing graph..
            #    Graph initialized!
            #    Processing index: vertex_index
            #        Start Index : 0
            #        End Index   : 38
            #        Chunk Count : 32
            #        Batch Size  : 2
            #    Batch Id: [30-32] thread start
            #    [...]
            #    Batch Id: [28-30]: {28-30} commit
            #    Batch Id: [28-30] thread end. Time taken: 31ms
            #    Restore complete: edge_index. Time taken: 36ms
            #    Graph shutdown!
            #    ==>null

    11. You can now exit the Gremlin promt and start Atlas service from Ambari UI.
        # gremlin> quit